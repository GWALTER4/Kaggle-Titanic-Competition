{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports titanic dataset\n",
    "titanicTraining = pd.read_csv('train.csv')\n",
    "titanicTest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Features That Are Not Important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the 'PassengerId' column for use in the Kaggle submission\n",
    "passengerId = titanicTest['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops features that are not important\n",
    "titanicTraining.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "titanicTest.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new feature for family size, number of sibling/spouse + number of parents/children + themselves\n",
    "titanicTraining['FamilySize'] = titanicTraining['SibSp'] + titanicTraining['Parch'] + 1\n",
    "titanicTest['FamilySize'] = titanicTest['SibSp'] + titanicTest['Parch'] + 1\n",
    "\n",
    "# Creates a new feature for if someone is alone, perhaps more likely to survive if no family to take care of?\n",
    "titanicTraining['IsAlone'] = 0\n",
    "titanicTest['IsAlone'] = 0\n",
    "\n",
    "titanicTraining.loc[titanicTraining['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "titanicTest.loc[titanicTest['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "# The column 'SibSp' now seems redundant so it can be dropped\n",
    "titanicTraining.drop(['SibSp'], axis=1, inplace=True)\n",
    "titanicTest.drop(['SibSp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an imputer that replaces NaN values with the median of the column values\n",
    "numeric_imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "# Creates an imputer that replaces NaN values with the most frequent of the column values\n",
    "categorical_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "# Applies the corresponding imputers to the dataset columns\n",
    "titanicTraining['Age'] = numeric_imputer.fit_transform(titanicTraining[['Age']])\n",
    "titanicTraining['Embarked'] = categorical_imputer.fit_transform(titanicTraining[['Embarked']])\n",
    "titanicTest['Age'] = numeric_imputer.fit_transform(titanicTest[['Age']])\n",
    "titanicTest['Fare'] = numeric_imputer.fit_transform(titanicTest[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the name of the columns with type 'object' in a list\n",
    "object_columns = ['Sex', 'Embarked']\n",
    "\n",
    "# Creates a one hot encoder that ignores classes not represented in the training data and returns a numpy array\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "\n",
    "# Applies one hot encoder to columns with categorical data\n",
    "encoded_data_train = pd.DataFrame(encoder.fit_transform(titanicTraining[object_columns]))\n",
    "encoded_data_test = pd.DataFrame(encoder.fit_transform(titanicTest[object_columns]))\n",
    "\n",
    "# Puts back index after it was removed by the encoder\n",
    "encoded_data_train.index = titanicTraining.index\n",
    "encoded_data_test.index = titanicTest.index\n",
    "\n",
    "# Removes the categorical columns\n",
    "titanicTrainingNumerical = titanicTraining.drop(object_columns, axis=1)\n",
    "titanicTestNumerical = titanicTest.drop(object_columns, axis=1)\n",
    "\n",
    "# Concatenates the numerical columns with the encoded categorical columns\n",
    "titanicTrainingEncoded = pd.concat([titanicTrainingNumerical, encoded_data_train], axis=1)\n",
    "titanicTestEncoded = pd.concat([titanicTestNumerical, encoded_data_test], axis=1)\n",
    "\n",
    "# Rearranges the columns\n",
    "titanicTrainingEncoded = titanicTrainingEncoded[['Age','Parch','FamilySize','IsAlone','Fare','Pclass',0,1,2,3,4,'Survived']]\n",
    "titanicTestEncoded = titanicTestEncoded[['Age','Parch','FamilySize','IsAlone','Fare','Pclass',0,1,2,3,4]]\n",
    "\n",
    "# Renames the encoded categorical columns to be more recognisable\n",
    "titanicTrainingEncoded.rename(columns={0: 'Female', 1: 'Male', 2:'Cherbourg', 3:'Queenstown', 4:'Southampton'}, inplace=True)\n",
    "titanicTestEncoded.rename(columns={0: 'Female', 1: 'Male', 2:'Cherbourg', 3:'Queenstown', 4:'Southampton'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Target Column From Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates the target column from the rest of the data\n",
    "trainingY = titanicTrainingEncoded['Survived'].map({True:1, False:0})\n",
    "trainingX = titanicTrainingEncoded.drop('Survived', axis=1)\n",
    "testX = titanicTestEncoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7947060185751619"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a new default KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Creates a min-max scaler and transforms the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_trainingX = pd.DataFrame(scaler.fit_transform(trainingX), columns=trainingX.columns)\n",
    "scaled_testX = pd.DataFrame(scaler.fit_transform(testX), columns=testX.columns)\n",
    "\n",
    "# Performs 5-fold corss validation on the dataset and records the scores\n",
    "cross_val_knn = cross_val_score(knn, scaled_trainingX, trainingY, cv=5)\n",
    "\n",
    "# Prints the averages of the scores\n",
    "np.mean(cross_val_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.57 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [3, 5, 11, 19], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a new default logistic regression classifier\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "# Creates the potential parameters for the classifier\n",
    "neighbors = [3,5,11,19]\n",
    "weights = ['uniform', 'distance']\n",
    "metric = ['euclidean', 'manhattan'] \n",
    "\n",
    "knn_parameters = {'n_neighbors': neighbors , 'weights': weights, 'metric': metric}\n",
    "\n",
    "# Runs a search over the parameter values for the classifier\n",
    "grid_knn = GridSearchCV(knn2, knn_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "%time grid_knn.fit(scaled_trainingX, trainingY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8024691358024691,\n",
       " {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'uniform'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the best score and best parameters from the search\n",
    "grid_knn.best_score_, grid_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7935070982311785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a default logistic regression classifier\n",
    "lgr = LogisticRegression(random_state=42)\n",
    "\n",
    "# Performs 5-fold corss validation on the dataset and records the scores \n",
    "cross_val_lgr = cross_val_score(lgr, trainingX, trainingY, cv=5)\n",
    "\n",
    "# Prints the averages of the scores\n",
    "np.mean(cross_val_lgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 397 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'C': array([1.00000e+00, 2.78256e+00, 7.74264e+00, 2.15443e+01, 5.99484e+01,\n",
       "       1.66810e+02, 4.64159e+02, 1.29155e+03, 3.59381e+03, 1.00000e+04]), 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a new default logistic regression classifier\n",
    "lgr2 = LogisticRegression(random_state=42)\n",
    "\n",
    "# Creates the potential parameters for the classifier\n",
    "penalties = ['l1', 'l2']\n",
    "Cs = np.logspace(0, 4, 10)\n",
    "\n",
    "regression_parameters = {'C': Cs,'penalty': penalties}\n",
    "\n",
    "# Runs a search over the parameter values for the classifier\n",
    "grid_lgr = GridSearchCV(lgr2, regression_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "%time grid_lgr.fit(trainingX, trainingY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7934904601571269, {'C': 1.0, 'penalty': 'l2'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the best score and best parameters from the search\n",
    "grid_lgr.best_score_, grid_lgr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8025084873431929"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a random forest classifier that uses 100 trees (suggested default)\n",
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Performs 5-fold corss validation on the dataset and records the scores \n",
    "cross_val_rfc = cross_val_score(rfc,trainingX, trainingY, cv=5)\n",
    "\n",
    "# Prints the averages of the scores (may get better result after optimising hyperparameters)\n",
    "np.mean(cross_val_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=64, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), 'max_features': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a new default random forest classifier\n",
    "rfc2 = RandomForestClassifier(n_estimators=64, random_state=42)\n",
    "\n",
    "# Creates the potential parameters for the classifier\n",
    "depths = np.arange(1,11)\n",
    "features = np.arange(1, trainingX.shape[1] + 1)\n",
    "\n",
    "tree_parameters = {'max_depth': depths,'max_features': features}\n",
    "\n",
    "# Runs a search over the parameter values for the classifier\n",
    "grid_rfc = GridSearchCV(rfc2, tree_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "%time grid_rfc.fit(trainingX, trainingY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8428731762065096, {'max_depth': 8, 'max_features': 9})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the best score and best parameters from the search\n",
    "grid_rfc.best_score_, grid_rfc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier (Ensemble Method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='manhattan')\n",
    "lgr_best = LogisticRegression(C=1.0, penalty='l2')\n",
    "rfc_best = RandomForestClassifier(n_estimators=64, max_depth=9, max_features=9, random_state=42)\n",
    "\n",
    "models = [('knn', knn_best), ('lgr', lgr_best), ('rfc', rfc_best)]\n",
    "\n",
    "ensemble = VotingClassifier(models, voting='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "knn_final = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='manhattan') \n",
    "knn_final.fit(scaled_trainingX, trainingY)\n",
    "predictions = knn_final.predict(scaled_testX)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': passengerId, 'Survived': predictions})\n",
    "output.to_csv('knn.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lgr_final = LogisticRegression(C=1.0, penalty='l2')\n",
    "lgr_final.fit(trainingX, trainingY)\n",
    "predictions = lgr_final.predict(testX)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': passengerId, 'Survived': predictions})\n",
    "output.to_csv('logisticRegression.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "rfc_final = RandomForestClassifier(n_estimators=64, max_depth=9, max_features=9, random_state=42)\n",
    "rfc_final.fit(trainingX, trainingY)\n",
    "predictions = rfc_final.predict(testX)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': passengerId, 'Survived': predictions})\n",
    "output.to_csv('randomForest.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ensemble.fit(trainingX, trainingY)\n",
    "predictions = ensemble.predict(testX)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': passengerId, 'Survived': predictions})\n",
    "output.to_csv('ensembleHard.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
